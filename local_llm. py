# BERT Text Classification Project - Complete Implementation

# Part 1: Setup
# Part 1.1: Import the Necessary Libraries

import wget
import gzip, shutil
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score

# Part 1.2: Check for GPU availability
if torch.cuda.is_available():
    device = torch.device("cuda")
    print('There are %d GPU(s) available.' % torch.cuda.device_count())
    print('We will use the GPU:', torch.cuda.get_device_name(0))
else:
    print('No GPU available, using the CPU instead.')
    device = torch.device("cpu")

# Part 2: Loading the Emotions Dataset and Preprocessing it [20 Marks]

# Download the dataset
wget.download('https://saref.github.io/teaching/MIE1626/Emotions_small.csv')

# Read the CSV file
df = pd.read_csv('Emotions_small.csv', index_col=0)
print(f"Number of messages in the dataset: {len(df)}")
print("\n10 random rows from the DataFrame:")
print(df.sample(10))

# Limit dataset to first 1370 messages
df = df.head(1370)
print(f"\nDataset limited to: {len(df)} messages")

# Check for missing values and duplicates
print(f"\nMissing values per column:\n{df.isnull().sum()}")
print(f"\nNumber of duplicated rows: {df.duplicated().sum()}")

# Remove missing values and duplicates
df = df.dropna()
df = df.drop_duplicates()
print(f"\nNumber of remaining messages after cleaning: {len(df)}")

# Create label mapping
unique_labels = df['label'].unique()
print(f"\nUnique labels: {unique_labels}")

# Create label dictionary
label_dict = {
    'sadness': 0,
    'joy': 1,
    'love': 2,
    'anger': 3,
    'fear': 4,
    'surprise': 5
}

# Map string labels to numerical labels
df['label'] = df['label'].map(label_dict)

# Visualize class distributions
plt.figure(figsize=(10, 6))
label_counts = df['label'].value_counts().sort_index()
emotion_names = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']
plt.bar(emotion_names, label_counts)
plt.xlabel('Emotion Categories')
plt.ylabel('Frequency')
plt.title('Distribution of Emotions in the Dataset')
for i, v in enumerate(label_counts):
    plt.text(i, v + 5, str(v), ha='center')
plt.show()

print(f"\nHighest frequency: {emotion_names[label_counts.idxmax()]} with {label_counts.max()} messages")
print(f"Lowest frequency: {emotion_names[label_counts.idxmin()]} with {label_counts.min()} messages")

# Part 3: Preparing Text Data for BERT [25 Marks]

# Part 3.1: Character and Word tokenization [10 Marks]

# Get the first Twitter message
first_message = df.iloc[0]['text']
print(f"\nFirst Twitter message: {first_message}")

# Character tokenization
char_tokens = list(first_message)
print(f"\nCharacter tokens: {char_tokens}")
print(f"Number of character tokens: {len(char_tokens)}")

# Create character to integer mapping
char_vocab = {char: idx for idx, char in enumerate(set(char_tokens))}
print(f"\nLength of character dictionary: {len(char_vocab)}")

# Convert to numerical format and one-hot encode
char_indices = [char_vocab[char] for char in char_tokens]
char_tensor = torch.tensor(char_indices)
one_hot_chars = F.one_hot(char_tensor, num_classes=len(char_vocab))
print(f"\nShape of one-hot vector: {one_hot_chars.shape}")

# Word tokenization
word_tokens = first_message.split()
print(f"\nWord tokens: {word_tokens}")
print(f"Number of word tokens: {len(word_tokens)}")

print("\nDrawbacks of Character and Word Tokenization:")
print("1. Character tokenization: Creates very long sequences, loses semantic meaning of words")
print("2. Word tokenization: Cannot handle out-of-vocabulary words, large vocabulary size")

# Part 3.2: BERT Tokenizer [15 Marks]

# Load BERT tokenizer
print('\nLoading BERT tokenizer...')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Report tokenizer properties
print(f"\nVocabulary size: {tokenizer.vocab_size}")
print(f"Maximum input length: {tokenizer.model_max_length}")
print(f"Model input names: {tokenizer.model_input_names}")

# Find maximum sentence length
max_len_dataset = 0
for text in df['text']:
    tokens = tokenizer.tokenize(text)
    max_len_dataset = max(max_len_dataset, len(tokens) + 2)  # +2 for [CLS] and [SEP]

print(f"\nMaximum sentence length in dataset (including [CLS] and [SEP]): {max_len_dataset}")

# Calculate MAX_LEN as next power of 2
import math
MAX_LEN = 2 ** math.ceil(math.log2(max_len_dataset))
print(f"MAX_LEN (next power of 2): {MAX_LEN}")

# Encode all sentences
input_ids = []
attention_masks = []

for text in df['text']:
    # Encode the text
    encoded = tokenizer.encode(
        text,
        add_special_tokens=True,
        truncation=True,
        max_length=MAX_LEN,
        padding='max_length'
    )
    input_ids.append(encoded)
    
    # Create attention mask
    attention_mask = [1 if token_id != 0 else 0 for token_id in encoded]
    attention_masks.append(attention_mask)

print(f"\nTotal encoded sequences: {len(input_ids)}")

# Print 300th message and its encoding
print(f"\n300th Twitter message: {df.iloc[299]['text']}")
print(f"Token IDs: {input_ids[299]}")
print(f"Attention mask: {attention_masks[299]}")

# Part 4: Feeding the Data into BERT [35 Marks]

# Part 4.1: Training and Validation Split [8 Marks]
from datasets import Dataset

# Get labels
labels = df['label'].values

# Split the data
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(
    input_ids, labels, test_size=0.1, random_state=41
)

train_masks, validation_masks = train_test_split(
    attention_masks, test_size=0.1, random_state=41
)

print(f"\nTraining set size: {len(train_inputs)}")
print(f"Validation set size: {len(validation_inputs)}")

# Create training dataset
train_df = pd.DataFrame({
    'input_ids': train_inputs,
    'attention_mask': train_masks,
    'label': train_labels
})
train_dataset = Dataset.from_pandas(train_df)

# Create validation dataset
val_df = pd.DataFrame({
    'input_ids': validation_inputs,
    'attention_mask': validation_masks,
    'label': validation_labels
})
val_dataset = Dataset.from_pandas(val_df)

# Part 4.2: Train the BertForSequenceClassification Model [15 Marks]
from transformers import BertForSequenceClassification, Trainer, TrainingArguments

# Load BERT model
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=6,
    output_attentions=False,
    output_hidden_states=False
)

# Move model to device
model = model.to(device)

# Define compute_metrics function
def compute_metrics(eval_prediction):
    labels = eval_prediction.label_ids
    preds_list = eval_prediction.predictions
    preds = preds_list.argmax(-1)
    f1 = f1_score(labels, preds, average="weighted")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}

# Define training arguments
training_args = TrainingArguments(
    output_dir="bert-base-uncased-finetuned-emotion",
    num_train_epochs=10,  # Or 5 if time/resources are limited
    learning_rate=2e-5,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    eval_strategy="epoch",
    disable_tqdm=False,
    report_to="none",
    logging_steps=len(train_inputs) // 64,
    push_to_hub=False,
    log_level="error"
)

# Define trainer
import os
os.environ['WANDB_DISABLED'] = 'true'

trainer = Trainer(
    model=model,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train the model
print("\nStarting training...")
trainer.train()

# Part 4.3: Model Evaluation [12 Marks]

# Extract training history
log_history = trainer.state.log_history

# Prepare data for table and plot
epochs = []
train_losses = []
val_losses = []
accuracies = []
f1_scores = []

for i in range(0, len(log_history), 2):
    if i+1 < len(log_history):
        epochs.append(log_history[i]['epoch'])
        train_losses.append(log_history[i]['loss'])
        val_losses.append(log_history[i+1]['eval_loss'])
        accuracies.append(log_history[i+1]['eval_accuracy'])
        f1_scores.append(log_history[i+1]['eval_f1'])

# Create results table
results_df = pd.DataFrame({
    'Epoch': epochs,
    'Training Loss': train_losses,
    'Validation Loss': val_losses,
    'Accuracy': accuracies,
    'F1 Score': f1_scores
})
print("\nTraining Results:")
print(results_df)

# Plot training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(epochs, train_losses, label='Training Loss', marker='o')
plt.plot(epochs, val_losses, label='Validation Loss', marker='s')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss over Epochs')
plt.legend()
plt.grid(True)
plt.show()

print("\nInterpretation of learning curves:")
print("- The training loss decreases consistently, showing the model is learning")
print("- The validation loss also decreases, indicating good generalization")
print("- The gap between training and validation loss suggests some overfitting")

# Get predictions on validation set
predictions = trainer.predict(val_dataset)
pred_labels = predictions.predictions.argmax(-1)

# Generate classification report
print("\nClassification Report:")
print(classification_report(validation_labels, pred_labels, 
                          target_names=emotion_names))

# Generate confusion matrix
cm = confusion_matrix(validation_labels, pred_labels)
plt.figure(figsize=(8, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=emotion_names)
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

print("\nInterpretation of results:")
print("- The model shows good performance across most emotion categories")
print("- Some emotions may be confused with others due to subtle differences")
print("- The diagonal values in confusion matrix show correct predictions")

# Part 5: Performance on a Test Set (Optional) [10 Marks BONUS]

# Save the model
output_dir = './model_save/'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

print(f"\nSaving model to {output_dir}")
model_to_save = model.module if hasattr(model, 'module') else model
model_to_save.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

# Check file sizes
import os
files = os.listdir(output_dir)
file_sizes = [(f, os.path.getsize(os.path.join(output_dir, f))) for f in files]
largest_file = max(file_sizes, key=lambda x: x[1])
print(f"\nLargest file: {largest_file[0]} with size: {largest_file[1] / (1024*1024):.2f} MB")
print("This file represents the trained model weights and parameters")

# Test on new messages
test_messages = [
    "If karma does not hit you in the face, I will.",
    "The toys R us advert makes me cry."
]

print("\nPredictions on test messages:")
for message in test_messages:
    # Tokenize
    inputs = tokenizer(message, padding=True, truncation=True, 
                      max_length=MAX_LEN, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # Get prediction
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_label = predictions.argmax(-1).item()
    
    # Map back to emotion
    reverse_label_dict = {v: k for k, v in label_dict.items()}
    predicted_emotion = reverse_label_dict[predicted_label]
    
    print(f"\nMessage: '{message}'")
    print(f"Predicted emotion: {predicted_emotion}")
    print(f"Confidence: {predictions.max().item():.2f}")